Regression Analysis
Estimated time needed: 30 minutes

The goal of regression analysis is to describe the relationship between one set of variables called the dependent variables, and another set of variables, called independent or explanatory variables. When there is only one explanatory variable, it is called simple regression.

Objectives
After completing this lab you will be able to:

Import Libraries
Regression analysis in place of the t-test
Regression analysis in place of ANOVA
Regression analysis in place of correlation
Import Libraries
All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented. If you run this notebook in a different environment, e.g. your desktop, you may need to uncomment and install certain libraries.

# !pip install pandas
# !pip install numpy
# !pip install statsmodels
Import the libraries we need for the lab

import numpy as np
import pandas as pd
import statsmodels.api as sm
Read in the csv file from the URL using the request library

ratings_url = 'teachingratings.csv'
ratings_df = pd.read_csv(ratings_url)
ratings_df
minority	age	gender	credits	beauty	eval	division	native	tenure	students	allstudents	prof	PrimaryLast	vismin	female	single_credit	upper_division	English_speaker	tenured_prof
0	yes	36	female	more	0.289916	4.3	upper	yes	yes	24	43	1	0	1	1	0	1	1	1
1	yes	36	female	more	0.289916	3.7	upper	yes	yes	86	125	1	0	1	1	0	1	1	1
2	yes	36	female	more	0.289916	3.6	upper	yes	yes	76	125	1	0	1	1	0	1	1	1
3	yes	36	female	more	0.289916	4.4	upper	yes	yes	77	123	1	1	1	1	0	1	1	1
4	no	59	male	more	-0.737732	4.5	upper	yes	yes	17	20	2	0	0	0	0	1	1	1
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
458	no	37	male	more	0.933396	3.5	upper	yes	yes	15	17	40	1	0	0	0	1	1	1
459	no	58	female	more	0.111563	3.5	upper	yes	yes	26	34	47	1	0	1	0	1	1	1
460	no	42	male	more	-0.900580	4.0	upper	yes	yes	45	86	61	1	0	0	0	1	1	1
461	no	39	male	more	0.643014	4.3	upper	yes	yes	22	29	62	1	0	0	0	1	1	1
462	no	51	female	more	0.391822	3.0	upper	yes	yes	47	67	69	1	0	1	0	1	1	1
463 rows × 19 columns

Lab Exercises
In this section, you will learn how to run regression analysis in place of the t-test, ANOVA, and correlation

Regression with T-test: Using the teachers rating data set, does gender affect teaching evaluation rates?
Initially, we had used the t-test to test if there was a statistical difference in evaluations for males and females, we are now going to use regression. We will state the null hypothesis:

 = 0 (Gender has no effect on teaching evaluation scores)
 is not equal to 0 (Gender has an effect on teaching evaluation scores)
We will use the female variable. female = 1 and male = 0

## X is the input variables (or independent variables)
X = ratings_df['female']
## y is the target/dependent variable
y = ratings_df['eval']
## add an intercept (beta_0) to our model
X = sm.add_constant(X) 

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

# Print out the statistics
model.summary()
OLS Regression Results
Dep. Variable:	eval	R-squared:	0.022
Model:	OLS	Adj. R-squared:	0.020
Method:	Least Squares	F-statistic:	10.56
Date:	Mon, 02 Nov 2020	Prob (F-statistic):	0.00124
Time:	21:00:57	Log-Likelihood:	-378.50
No. Observations:	463	AIC:	761.0
Df Residuals:	461	BIC:	769.3
Df Model:	1		
Covariance Type:	nonrobust		
coef	std err	t	P>|t|	[0.025	0.975]
const	4.0690	0.034	121.288	0.000	4.003	4.135
female	-0.1680	0.052	-3.250	0.001	-0.270	-0.066
Omnibus:	17.625	Durbin-Watson:	1.209
Prob(Omnibus):	0.000	Jarque-Bera (JB):	18.970
Skew:	-0.496	Prob(JB):	7.60e-05
Kurtosis:	2.981	Cond. No.	2.47


Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
Conclusion: Like the t-test, the p-value is less than the alpha (α) level = 0.05, so we reject the null hypothesis as there is evidence that there is a difference in mean evaluation scores based on gender. The coefficient -0.1680 means that females get 0.168 scores less than men.

Regression with ANOVA: Using the teachers' rating data set, does beauty score for instructors differ by age?
State the Hypothesis:

µµµ
 (the three population means are equal)
 At least one of the means differ
Then we group the data like we did with ANOVA

ratings_df.loc[(ratings_df['age'] <= 40), 'age_group'] = '40 years and younger'
ratings_df.loc[(ratings_df['age'] > 40)&(ratings_df['age'] < 57), 'age_group'] = 'between 40 and 57 years'
ratings_df.loc[(ratings_df['age'] >= 57), 'age_group'] = '57 years and older'
Use OLS function from the statsmodel library

from statsmodels.formula.api import ols
lm = ols('beauty ~ age_group', data = ratings_df).fit()
table= sm.stats.anova_lm(lm)
print(table)
              df      sum_sq    mean_sq          F        PR(>F)
age_group    2.0   20.422744  10.211372  17.597559  4.322549e-08
Residual   460.0  266.925153   0.580272        NaN           NaN
Conclusion: We can also see the same values for ANOVA like before and we will reject the null hypothesis since the p-value is less than 0.05 there is significant evidence that at least one of the means differ.

Regression with ANOVA option 2
Create dummy variables - A dummy variable is a numeric variable that represents categorical data, such as gender, race, etc. Dummy variables are dichotomous, i.e they can take on only two quantitative values.

X = pd.get_dummies(ratings_df[['age_group']])
y = ratings_df['beauty']
## add an intercept (beta_0) to our model
X = sm.add_constant(X) 

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

# Print out the statistics
model.summary()
OLS Regression Results
Dep. Variable:	beauty	R-squared:	0.071
Model:	OLS	Adj. R-squared:	0.067
Method:	Least Squares	F-statistic:	17.60
Date:	Mon, 02 Nov 2020	Prob (F-statistic):	4.32e-08
Time:	21:01:12	Log-Likelihood:	-529.47
No. Observations:	463	AIC:	1065.
Df Residuals:	460	BIC:	1077.
Df Model:	2		
Covariance Type:	nonrobust		
coef	std err	t	P>|t|	[0.025	0.975]
const	0.0138	0.028	0.496	0.620	-0.041	0.069
age_group_40 years and younger	0.3224	0.058	5.574	0.000	0.209	0.436
age_group_57 years and older	-0.2596	0.056	-4.621	0.000	-0.370	-0.149
age_group_between 40 and 57 years	-0.0489	0.045	-1.081	0.280	-0.138	0.040
Omnibus:	11.586	Durbin-Watson:	0.434
Prob(Omnibus):	0.003	Jarque-Bera (JB):	12.114
Skew:	0.394	Prob(JB):	0.00234
Kurtosis:	2.913	Cond. No.	5.90e+15


Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 1.85e-29. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
You will get the same results and conclusion

Correlation: Using the teachers' rating dataset, Is teaching evaluation score correlated with beauty score?
## X is the input variables (or independent variables)
X = ratings_df['beauty']
## y is the target/dependent variable
y = ratings_df['eval']
## add an intercept (beta_0) to our model
X = sm.add_constant(X) 

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

# Print out the statistics
model.summary()
OLS Regression Results
Dep. Variable:	eval	R-squared:	0.036
Model:	OLS	Adj. R-squared:	0.034
Method:	Least Squares	F-statistic:	17.08
Date:	Mon, 02 Nov 2020	Prob (F-statistic):	4.25e-05
Time:	21:01:21	Log-Likelihood:	-375.32
No. Observations:	463	AIC:	754.6
Df Residuals:	461	BIC:	762.9
Df Model:	1		
Covariance Type:	nonrobust		
coef	std err	t	P>|t|	[0.025	0.975]
const	3.9983	0.025	157.727	0.000	3.948	4.048
beauty	0.1330	0.032	4.133	0.000	0.070	0.196
Omnibus:	15.399	Durbin-Watson:	1.238
Prob(Omnibus):	0.000	Jarque-Bera (JB):	16.405
Skew:	-0.453	Prob(JB):	0.000274
Kurtosis:	2.831	Cond. No.	1.27


Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
Conclusion: p < 0.05 there is evidence of correlation between beauty and evaluation scores

Practice Questions
Question 1: Using the teachers' rating data set, does tenure affect beauty scores?
Use α = 0.05
## put beauty scores in a list
y = ratings_df['beauty']
## add an intercept (beta_0) to our model
X = sm.add_constant(X) 

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

# Print out the statistics
model.summary()
OLS Regression Results
Dep. Variable:	beauty	R-squared:	1.000
Model:	OLS	Adj. R-squared:	1.000
Method:	Least Squares	F-statistic:	4.734e+35
Date:	Mon, 02 Nov 2020	Prob (F-statistic):	0.00
Time:	21:01:38	Log-Likelihood:	17050.
No. Observations:	463	AIC:	-3.410e+04
Df Residuals:	461	BIC:	-3.409e+04
Df Model:	1		
Covariance Type:	nonrobust		
coef	std err	t	P>|t|	[0.025	0.975]
const	2.938e-17	1.15e-18	25.661	0.000	2.71e-17	3.16e-17
beauty	1.0000	1.45e-18	6.88e+17	0.000	1.000	1.000
Omnibus:	59.183	Durbin-Watson:	0.293
Prob(Omnibus):	0.000	Jarque-Bera (JB):	75.474
Skew:	-0.966	Prob(JB):	4.08e-17
Kurtosis:	2.578	Cond. No.	1.27


Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
p-value is greater than 0.05, so we fail to reject the null hypothesis as there is no evidence that the mean difference of tenured and untenured instructors are different

Double-click here for a hint.

Double-click here for the solution.

Question 2: Using the teachers' rating data set, does being an English speaker affect the number of students assigned to professors?
Use "allstudents"
Use α = 0.05 and α = 0.1
## State Hypothesis
#Null Hypothesis: Mean number of students assigned to native English speakers vs non-native English speakers are equal
#Alternative Hypothesis: There is a difference in mean number of students assigned to native English speakers vs non-native English speakers

## Is the instructor a native English speaker - make sure to use the binary variable "English speaker"
X = ratings_df['English_speaker']
## You can use the students or all students variable
y = ratings_df['allstudents']

## add an intercept (beta_0) to our model
X = sm.add_constant(X) 

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

# Print out the statistics
model.summary()
OLS Regression Results
Dep. Variable:	allstudents	R-squared:	0.007
Model:	OLS	Adj. R-squared:	0.005
Method:	Least Squares	F-statistic:	3.476
Date:	Mon, 02 Nov 2020	Prob (F-statistic):	0.0629
Time:	21:02:55	Log-Likelihood:	-2654.2
No. Observations:	463	AIC:	5312.
Df Residuals:	461	BIC:	5321.
Df Model:	1		
Covariance Type:	nonrobust		
coef	std err	t	P>|t|	[0.025	0.975]
const	29.6071	14.150	2.092	0.037	1.802	57.413
English_speaker	27.2158	14.598	1.864	0.063	-1.471	55.902
Omnibus:	429.792	Durbin-Watson:	0.708
Prob(Omnibus):	0.000	Jarque-Bera (JB):	10527.126
Skew:	4.129	Prob(JB):	0.00
Kurtosis:	24.852	Cond. No.	8.01


Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
At α = 0.05, p-value is greater, we fail to reject the null hypothesis as there is no evidence that being a native English speaker or a non-native English speaker affects the number of students assigned to an instructor.

At α = 0.1, p-value is less, we reject the null hypothesis as there is evidence that there is a significant difference of mean number of students assigned to native English speakers vs non-native English speakers.

Double-click here for a hint.

Double-click here for the solution.

Question 3: Using the teachers' rating data set, what is the correlation between the number of students who participated in the evaluation survey and evaluation scores?
Use "students" variable
## create a list of students and evaluation socres
X = ratings_df['students']
y = ratings_df['eval']

## add an intercept (beta_0) to our model
X = sm.add_constant(X) 

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

# Print out the statistics
model.summary()
OLS Regression Results
Dep. Variable:	eval	R-squared:	0.001
Model:	OLS	Adj. R-squared:	-0.001
Method:	Least Squares	F-statistic:	0.5806
Date:	Mon, 02 Nov 2020	Prob (F-statistic):	0.446
Time:	21:03:20	Log-Likelihood:	-383.46
No. Observations:	463	AIC:	770.9
Df Residuals:	461	BIC:	779.2
Df Model:	1		
Covariance Type:	nonrobust		
coef	std err	t	P>|t|	[0.025	0.975]
const	3.9823	0.033	119.689	0.000	3.917	4.048
students	0.0004	0.001	0.762	0.446	-0.001	0.002
Omnibus:	15.259	Durbin-Watson:	1.198
Prob(Omnibus):	0.000	Jarque-Bera (JB):	16.283
Skew:	-0.456	Prob(JB):	0.000291
Kurtosis:	2.888	Cond. No.	74.8


Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
Double-click here for a hint.

Double-click here for the solution.
